{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73ae98d4-015a-4a15-826e-51e3443d4062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scrape():\n",
    "    '''Scrapes several websites about Mars'''\n",
    "    \n",
    "    def init_browser():\n",
    "        '''Launches a Chrome browser using splinter and ChromeDriverManager'''\n",
    "        executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "        return Browser('chrome', **executable_path, headless=True)\n",
    "\n",
    "    def browse(url):\n",
    "        '''Function to scrape a single webpage'''\n",
    "        browser = init_browser()\n",
    "        browser.visit(url)\n",
    "\n",
    "        # Scrape html code\n",
    "        html = browser.html    \n",
    "\n",
    "        # Create BeautifulSoup object; parse with 'html.parser'\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # Quit browser\n",
    "        browser.quit()\n",
    "\n",
    "        return soup\n",
    "\n",
    "    # Create an empty dictionary to hold web scraping resutls\n",
    "    mars_scrape_dict = {}\n",
    "\n",
    "    # URL for NASA Mars News Site \n",
    "    nasa_url = \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\n",
    "\n",
    "    # Scrape html of nasa_url\n",
    "    nasa_soup = browse(nasa_url)\n",
    "\n",
    "    # Find title and teaser text of first article and store them to variables\n",
    "    first_article_title = nasa_soup.find(\"div\", class_=\"bottom_gradient\").text\n",
    "    first_article_teaser = nasa_soup.find(\"div\", class_=\"article_teaser_body\").text\n",
    "\n",
    "    # Append the article variables to mars_scrape_dict\n",
    "    mars_scrape_dict[\"article_title\"] = first_article_title\n",
    "    mars_scrape_dict[\"article_teaser\"] = first_article_teaser\n",
    "\n",
    "    # URL to JPL Mars image website\n",
    "    jpl_url = \"https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/index.html\"\n",
    "\n",
    "    # Scrape jpl_url\n",
    "    jpl_soup = browse(jpl_url)\n",
    "\n",
    "    # Scrape the link to the featured image\n",
    "    jpl_image_link = jpl_soup.find(\"a\", class_=\"showimg fancybox-thumbs\")[\"href\"]\n",
    "\n",
    "    # Assign the full image url to a variable\n",
    "    featured_image_url = f\"https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/{jpl_image_link}\"\n",
    "\n",
    "    mars_scrape_dict[\"featured_image\"] = featured_image_url\n",
    "\n",
    "    # Scrape url using pandas and assign scraped tables to a variable\n",
    "    space_facts_url = \"https://space-facts.com/mars/\"\n",
    "    tables = pd.read_html(space_facts_url)\n",
    "\n",
    "    # Create a dataframe from the first table\n",
    "    stats_table_df = pd.DataFrame(tables[0])\n",
    "\n",
    "    # Rename the columns\n",
    "    stats_table_df = stats_table_df.rename(columns={0:\"\", 1:\"values\"}, inplace=False)\n",
    "\n",
    "    # Set the index to the first column\n",
    "    stats_table_new_index_df = stats_table_df.set_index(keys=\"\")\n",
    "\n",
    "    # Generate html code for the table and remove \"\\n\" from the code\n",
    "    stats_table_html = stats_table_new_index_df.to_html()\n",
    "\n",
    "    stats_table_html = stats_table_html.replace(\"\\n\", \"\")\n",
    "\n",
    "    # Append the table to mars_scrape_dict\n",
    "    mars_scrape_dict[\"table_html\"] = stats_table_html\n",
    "\n",
    "    # USGS Mars website url\n",
    "    usgs_url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "\n",
    "    # Initiate browswer instance\n",
    "    usgs_browser = init_browser()\n",
    "    usgs_browser.visit(usgs_url)\n",
    "\n",
    "    # Scrape image Mars hemispheres pages\n",
    "    hemispheres = [\"Cerberus\", \"Schiaparelli\", \"Syrtis\", \"Valles\"]\n",
    "    soup_objects = []\n",
    "\n",
    "    for hemisphere in hemispheres:\n",
    "        usgs_browser.links.find_by_partial_text(hemisphere).click()\n",
    "\n",
    "        # Scrape html code\n",
    "        usgs_html = usgs_browser.html    \n",
    "\n",
    "        # Create BeautifulSoup object; parse with 'html.parser'\n",
    "        usgs_soup = BeautifulSoup(usgs_html, \"html.parser\")\n",
    "\n",
    "        # Assign BeautifulSoup object to unique variable; print variable name\n",
    "        locals()[\"soup_\" + hemisphere] = usgs_soup\n",
    "\n",
    "        # Append soup object to list\n",
    "        soup_objects.append(locals()[\"soup_\" + hemisphere])\n",
    "\n",
    "        # Go back to previous page\n",
    "        usgs_browser.back()\n",
    "\n",
    "    usgs_browser.quit()\n",
    "\n",
    "    # Find title and url for hemispheres images\n",
    "    hemispheres_list = []\n",
    "\n",
    "    for soup_object in soup_objects:\n",
    "\n",
    "        hemispheres_dict = {}\n",
    "\n",
    "        title = soup_object.find(\"h2\", class_=\"title\").text.replace(\" Enhanced\", \"\")\n",
    "        img_url = soup_object.find_all(\"a\", target=\"_blank\")[3][\"href\"]\n",
    "\n",
    "        hemispheres_dict[\"title\"] = title\n",
    "        hemispheres_dict[\"img_url\"] = img_url\n",
    "\n",
    "        hemispheres_list.append(hemispheres_dict)\n",
    "\n",
    "    mars_scrape_dict[\"hemispheres\"] = hemispheres_list\n",
    "\n",
    "    return mars_scrape_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
